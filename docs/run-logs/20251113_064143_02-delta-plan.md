# Phase 2 — Extension Plan (Additive, Minimal-Change)

**RUN-ID**: `20251113_064143`  
**Timestamp**: 2025-11-13T06:41:43Z  
**Phase**: 2 — Extension Plan

---

## DELTA PLAN

### A) ETL Fallbacks (CSV + Optional Google Sheets)

#### Scope
Add manual CSV import as fallback when platform APIs fail or for historical data.

#### Implementation Plan
1. **Backend API**
   - `POST /api/v1/etl/upload` - Accept CSV file upload
   - `GET /api/v1/etl/status` - Get import status/history
   - Validate CSV schema (Pydantic model matching `metrics_daily.csv` format)
   - Upsert into `listener_metrics` hypertable (or create `metrics_daily` if preferred)
   - Track imports in `etl_imports` table

2. **CSV Schema** (from requirements)
   ```
   day,episode_id,source,downloads,listeners,completion_rate,ctr,conversions,revenue_cents
   ```

3. **Database Changes**
   - Add `etl_imports` table (idempotent)
   - Add unique constraint on `(day, episode_id, source)` for upsert logic
   - Add `last_sync_watermark` column or reuse existing job-state storage

4. **Frontend**
   - CSV uploader component (drag-drop)
   - Reuse existing file upload patterns if any
   - Show import status/progress

5. **Google Sheets** (Optional, if project already uses Apps Script/Sheets)
   - Only if `src/integrations/google_workspace.py` exists and is used
   - Add Sheets import endpoint (defer if not present)

#### Files to Create/Modify
- **New**: `src/api/etl.py` - ETL upload endpoints
- **New**: `src/etl/csv_importer.py` - CSV parsing/validation
- **New**: `frontend/components/etl/CsvUploader.tsx` - Upload UI
- **Modify**: `src/main.py` - Include ETL router
- **Migration**: `migrations/20251113_064143/01_detect_and_add.sql` - Add `etl_imports` table

#### Feature Flag
- `ENABLE_ETL_CSV_UPLOAD` (default: OFF)

---

### B) Deal Pipeline & IO (Only Missing Stages/Fields/Flows)

#### Scope
Extend `campaigns` table with pipeline stages and add IO bookings table.

#### Implementation Plan
1. **Deal Pipeline Extension**
   - Add `stage` column to `campaigns` table (if not exists)
   - Stages: `lead`, `qualified`, `proposal`, `negotiation`, `won`, `lost`
   - Add `stage_changed_at` timestamp
   - Emit `deal.stage_changed` event

2. **IO Bookings Table**
   - Create `io_bookings` table:
     - `io_id` (PK)
     - `deal_id` (FK to campaigns) or `campaign_id`
     - `ad_unit_id` (FK, may need to create `ad_units` table)
     - `episode_id` (FK, nullable - can be unassigned)
     - `flight_start`, `flight_end` (timestamps)
     - `booked_impressions` (integer)
     - `booked_cpm_cents` (integer)
     - `promo_code` (text, nullable)
     - `vanity_url` (text, nullable)
     - `status` (scheduled, active, completed, cancelled)
     - `tenant_id` (FK)
     - Standard timestamps/metadata

3. **Ad Units Table** (if needed)
   - Create `ad_units` table:
     - `ad_unit_id` (PK)
     - `podcast_id` (FK)
     - `name` (e.g., "Pre-roll", "Mid-roll", "Post-roll")
     - `duration_seconds` (integer)
     - `position` (pre/mid/post)
     - `tenant_id` (FK)

4. **IO Wizard/Modal**
   - Select ad_unit(s)
   - Choose episode(s) or leave unassigned
   - Set flight_start/flight_end
   - Generate vanity URL and promo_code (if utilities exist)

5. **Vanity URL & Promo Code Generation**
   - If patterns exist: reuse
   - If not: add minimal utilities
   - Store in `io_bookings` table

6. **IO PDF Export**
   - Reuse existing `src/reporting/report_generator.py`
   - Add IO template if needed

#### Files to Create/Modify
- **New**: `src/api/io.py` - IO booking endpoints
- **New**: `frontend/components/io/IOWizard.tsx` - IO creation UI
- **Modify**: `src/api/attribution.py` or create `src/api/deals.py` - Deal pipeline endpoints
- **Migration**: `migrations/20251113_064143/01_detect_and_add.sql` - Add columns/tables

#### Feature Flag
- `ENABLE_DEAL_PIPELINE` (default: OFF)
- `ENABLE_IO_BOOKINGS` (default: OFF)

---

### C) Matchmaking Endpoint (`/api/match/recalculate`) — Only If Absent

#### Scope
Add advertiser-podcast matchmaking with scoring.

#### Implementation Plan
1. **Matches Table**
   - Create `matches` table:
     - `match_id` (PK)
     - `advertiser_id` (FK to sponsors or new advertisers table)
     - `podcast_id` (FK)
     - `score` (numeric 0-100)
     - `rationale` (text - human-readable explanation)
     - `signals` (JSONB - geo/demo/topic overlaps, historical lift, inventory fit, brand safety)
     - `created_at`, `updated_at`
     - `tenant_id` (FK)

2. **Matchmaking Algorithm**
   - Compute score 0-100 using signals:
     - Geo overlap (if available)
     - Demographic overlap (if available)
     - Topic overlap (if available)
     - Historical lift (if available)
     - Inventory fit (episode availability)
     - Basic brand safety (explicit content check)
   - Normalize inputs 0-1
   - Weighted sum to get final score

3. **API Endpoint**
   - `POST /api/match/recalculate`
   - Query params: `advertiser_id?`, `podcast_id?`
   - If both provided: recalc single match
   - If one provided: recalc all matches for that entity
   - If neither: recalc all matches (admin only)
   - Persist score + rationale to `matches` table
   - Respect org/tenant scoping

4. **Event Emission**
   - Emit `match.recalculated` event

#### Files to Create/Modify
- **New**: `src/api/match.py` - Matchmaking endpoints
- **New**: `src/matchmaking/engine.py` - Matchmaking algorithm
- **Migration**: `migrations/20251113_064143/01_detect_and_add.sql` - Add `matches` table

#### Feature Flag
- `ENABLE_MATCHMAKING` (default: OFF)

---

### D) Dashboard Cards (Incremental, Reuse Existing Charts)

#### Scope
Add new dashboard cards without touching existing ones.

#### Implementation Plan
1. **Creator Dashboard Cards**
   - **Pacing vs Flight**: Show booked vs actual impressions/delivery
   - **Sponsor Revenue**: Sum of attributed revenue from `attribution_events`
   - **Makegoods Pending**: IO bookings with status "makegood" or under-delivery

2. **Advertiser/Agency Dashboard Cards**
   - **Audience Fit Summary**: Match scores from `matches` table
   - **Projected CPM**: Calculate from historical data
   - **Inventory Calendar**: Episodes with available ad slots

3. **Ops Dashboard Cards**
   - **Pipeline Forecast**: Deal pipeline stages distribution
   - **Win/Loss**: Won vs lost deals
   - **ETL Health**: Last ingestions from `etl_imports` table

4. **Implementation**
   - Reuse existing chart components (`TimeSeriesChart.tsx`, `FunnelChart.tsx`, `HeatmapChart.tsx`)
   - Add API endpoints for card data (e.g., `GET /api/v1/dashboard/creator`, `/api/v1/dashboard/advertiser`, `/api/v1/dashboard/ops`)
   - Add CSV export buttons if pattern exists

#### Files to Create/Modify
- **New**: `src/api/dashboard.py` - Dashboard data endpoints
- **New**: `frontend/components/dashboard/CreatorCards.tsx` - Creator cards
- **New**: `frontend/components/dashboard/AdvertiserCards.tsx` - Advertiser cards
- **New**: `frontend/components/dashboard/OpsCards.tsx` - Ops cards
- **Modify**: `frontend/app/page.tsx` or create dashboard pages

#### Feature Flag
- `ENABLE_NEW_DASHBOARD_CARDS` (default: OFF)

---

### E) Events/Logs (Emit Into Existing Bus/Table or Minimal New)

#### Scope
Emit new event types into existing event logger.

#### Implementation Plan
1. **New Event Types**
   - `deal.stage_changed` - When deal moves between stages
   - `io.scheduled` - When IO is created/scheduled
   - `io.delivered` - When IO flight completes
   - `etl.import_completed` - When CSV import finishes
   - `etl.error` - When CSV import fails
   - `match.recalculated` - When matchmaking runs

2. **Event Payloads**
   - Include IDs (deal_id, io_id, etc.)
   - Include from→to where applicable (stage_changed: `{"from": "lead", "to": "qualified"}`)

3. **Implementation**
   - Extend `src/telemetry/events.py` `EventLogger.log_event()` calls
   - Use existing event bus/table (no new infrastructure)

#### Files to Modify
- **Modify**: `src/api/deals.py` or `src/api/attribution.py` - Emit `deal.stage_changed`
- **Modify**: `src/api/io.py` - Emit `io.scheduled`, `io.delivered`
- **Modify**: `src/etl/csv_importer.py` - Emit `etl.import_completed`, `etl.error`
- **Modify**: `src/matchmaking/engine.py` - Emit `match.recalculated`

---

## DATA MODEL DELTAS

### New Tables

1. **`etl_imports`**
   ```sql
   CREATE TABLE IF NOT EXISTS etl_imports (
       import_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       tenant_id UUID REFERENCES tenants(tenant_id) ON DELETE CASCADE,
       source VARCHAR(100) NOT NULL, -- 'csv', 'google_sheets', etc.
       file_name VARCHAR(500),
       status VARCHAR(50) NOT NULL, -- 'pending', 'processing', 'completed', 'failed'
       records_imported INTEGER DEFAULT 0,
       records_failed INTEGER DEFAULT 0,
       error_message TEXT,
       started_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
       completed_at TIMESTAMP WITH TIME ZONE,
       metadata JSONB DEFAULT '{}'
   );
   ```

2. **`io_bookings`**
   ```sql
   CREATE TABLE IF NOT EXISTS io_bookings (
       io_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       tenant_id UUID REFERENCES tenants(tenant_id) ON DELETE CASCADE,
       campaign_id UUID REFERENCES campaigns(campaign_id) ON DELETE CASCADE,
       ad_unit_id UUID REFERENCES ad_units(ad_unit_id) ON DELETE SET NULL,
       episode_id UUID REFERENCES episodes(episode_id) ON DELETE SET NULL,
       flight_start TIMESTAMP WITH TIME ZONE NOT NULL,
       flight_end TIMESTAMP WITH TIME ZONE NOT NULL,
       booked_impressions INTEGER,
       booked_cpm_cents INTEGER,
       promo_code TEXT,
       vanity_url TEXT,
       status VARCHAR(50) NOT NULL DEFAULT 'scheduled',
       created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
       updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
       metadata JSONB DEFAULT '{}',
       CONSTRAINT valid_status CHECK (status IN ('scheduled', 'active', 'completed', 'cancelled', 'makegood'))
   );
   ```

3. **`ad_units`** (if needed)
   ```sql
   CREATE TABLE IF NOT EXISTS ad_units (
       ad_unit_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       tenant_id UUID REFERENCES tenants(tenant_id) ON DELETE CASCADE,
       podcast_id UUID REFERENCES podcasts(podcast_id) ON DELETE CASCADE,
       name VARCHAR(255) NOT NULL,
       duration_seconds INTEGER,
       position VARCHAR(50) NOT NULL, -- 'pre', 'mid', 'post'
       created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
       updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
       metadata JSONB DEFAULT '{}',
       CONSTRAINT valid_position CHECK (position IN ('pre', 'mid', 'post'))
   );
   ```

4. **`matches`**
   ```sql
   CREATE TABLE IF NOT EXISTS matches (
       match_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       tenant_id UUID REFERENCES tenants(tenant_id) ON DELETE CASCADE,
       advertiser_id UUID REFERENCES sponsors(sponsor_id) ON DELETE CASCADE,
       podcast_id UUID REFERENCES podcasts(podcast_id) ON DELETE CASCADE,
       score NUMERIC(5, 2) NOT NULL CHECK (score >= 0 AND score <= 100),
       rationale TEXT,
       signals JSONB DEFAULT '{}',
       created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
       updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
       UNIQUE(tenant_id, advertiser_id, podcast_id)
   );
   ```

### New Columns

1. **`campaigns.stage`** (if not exists)
   ```sql
   ALTER TABLE campaigns ADD COLUMN IF NOT EXISTS stage VARCHAR(50) DEFAULT 'lead';
   ALTER TABLE campaigns ADD COLUMN IF NOT EXISTS stage_changed_at TIMESTAMP WITH TIME ZONE;
   ```

2. **Unique Constraint for Metrics Daily** (if using `listener_metrics`)
   ```sql
   -- Add unique constraint if not exists (for upsert logic)
   -- Note: May need to check if this conflicts with hypertable structure
   ```

### New Indexes

1. **`etl_imports`**
   - `idx_etl_imports_tenant_id` ON `etl_imports(tenant_id)`
   - `idx_etl_imports_status` ON `etl_imports(status)`
   - `idx_etl_imports_started_at` ON `etl_imports(started_at DESC)`

2. **`io_bookings`**
   - `idx_io_bookings_tenant_id` ON `io_bookings(tenant_id)`
   - `idx_io_bookings_campaign_id` ON `io_bookings(campaign_id)`
   - `idx_io_bookings_episode_id` ON `io_bookings(episode_id)`
   - `idx_io_bookings_flight` ON `io_bookings(flight_start, flight_end)`
   - `idx_io_bookings_status` ON `io_bookings(status)`

3. **`ad_units`**
   - `idx_ad_units_tenant_id` ON `ad_units(tenant_id)`
   - `idx_ad_units_podcast_id` ON `ad_units(podcast_id)`

4. **`matches`**
   - `idx_matches_tenant_id` ON `matches(tenant_id)`
   - `idx_matches_advertiser_id` ON `matches(advertiser_id)`
   - `idx_matches_podcast_id` ON `matches(podcast_id)`
   - `idx_matches_score` ON `matches(score DESC)`

### RLS Policies

All new tables will have RLS enabled and tenant isolation policies:
- `tenant_isolation_<table_name>` policy using `current_setting('app.current_tenant')`

---

## GUARDRAILS

### Idempotent Migrations Pattern
- All migrations use `CREATE TABLE IF NOT EXISTS`
- All columns use `ADD COLUMN IF NOT EXISTS`
- All indexes use `CREATE INDEX IF NOT EXISTS`
- All policies use `CREATE POLICY IF NOT EXISTS` (or check existence first)

### Backups
- Document backup requirements before running migrations
- Use transaction blocks for atomicity
- Fail-fast with clear error messages

### Feature Toggles
- All new features behind flags (default OFF):
  - `ENABLE_ETL_CSV_UPLOAD`
  - `ENABLE_DEAL_PIPELINE`
  - `ENABLE_IO_BOOKINGS`
  - `ENABLE_MATCHMAKING`
  - `ENABLE_NEW_DASHBOARD_CARDS`

### Rollbacks
- Document rollback snippets for each delta
- Use `DROP TABLE IF EXISTS` for tables
- Use `ALTER TABLE DROP COLUMN IF EXISTS` for columns
- Use `DROP INDEX IF EXISTS` for indexes
- Use `DROP POLICY IF EXISTS` for policies

---

## ACCEPTANCE CRITERIA

### A) ETL Fallbacks
- ✅ CSV upload endpoint accepts valid CSV
- ✅ Invalid CSV returns 400 with clear errors
- ✅ Valid CSV upserts into `listener_metrics` (or `metrics_daily`)
- ✅ Import tracked in `etl_imports` table
- ✅ UI shows upload progress/status
- ✅ Feature flag controls access

### B) Deal Pipeline & IO
- ✅ `campaigns.stage` column exists (or added)
- ✅ Deal can move through stages (lead → qualified → proposal → negotiation → won/lost)
- ✅ `io_bookings` table exists
- ✅ IO can be created with ad_unit, episode, flight dates
- ✅ Promo code/vanity URL generated (if utilities exist)
- ✅ IO PDF export works (if report generator exists)
- ✅ Events emitted: `deal.stage_changed`, `io.scheduled`, `io.delivered`

### C) Matchmaking
- ✅ `matches` table exists
- ✅ `POST /api/match/recalculate` endpoint exists
- ✅ Score computed (0-100) with rationale
- ✅ Score persisted to `matches` table
- ✅ Tenant scoping respected
- ✅ Feature flag controls access
- ✅ Event emitted: `match.recalculated`

### D) Dashboard Cards
- ✅ New cards render with seed data
- ✅ Cards use existing chart components
- ✅ API endpoints return correct data
- ✅ CSV export works (if pattern exists)
- ✅ Feature flag controls access

### E) Events/Logs
- ✅ All new event types emitted
- ✅ Event payloads include IDs and from→to where applicable
- ✅ Events logged to existing event bus/table

---

## NO-GO CHANGES

### Prohibited Refactors
- ❌ Do NOT rename existing tables/columns
- ❌ Do NOT change existing table structures (only add)
- ❌ Do NOT modify existing API endpoints (only add new ones)
- ❌ Do NOT change existing frontend components (only add new ones)
- ❌ Do NOT remove existing functionality
- ❌ Do NOT change existing migration files

### Prohibited Moves
- ❌ Do NOT move existing files
- ❌ Do NOT reorganize directory structure
- ❌ Do NOT change import paths

### Prohibited Stack Swaps
- ❌ Do NOT change database (PostgreSQL/TimescaleDB)
- ❌ Do NOT change framework (FastAPI/Next.js)
- ❌ Do NOT change package managers (pip/npm)

---

## ACCEPTANCE TO PROCEED

✅ **Plan aligns with inventory and is strictly additive**

- All deltas are additive (no destructive changes)
- All migrations are idempotent
- All features are behind flags (default OFF)
- Rollback procedures documented
- Acceptance criteria defined
- No-go changes listed

**Status**: ✅ **READY TO PROCEED TO PHASE 3**

---

**Next Steps**: Phase 3 — Safe DB Migration Pack (idempotent SQL & RLS)
